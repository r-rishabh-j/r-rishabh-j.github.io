<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://r-rishabh-j.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://r-rishabh-j.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-09T21:59:27+00:00</updated><id>https://r-rishabh-j.github.io/feed.xml</id><title type="html">Rishabh Jain</title><subtitle>Rishabh Jain&apos;s porfolio </subtitle><entry><title type="html">Batched Speculative Decoding</title><link href="https://r-rishabh-j.github.io/blog/2026/specdec/" rel="alternate" type="text/html" title="Batched Speculative Decoding"/><published>2026-01-22T00:00:00+00:00</published><updated>2026-01-22T00:00:00+00:00</updated><id>https://r-rishabh-j.github.io/blog/2026/specdec</id><content type="html" xml:base="https://r-rishabh-j.github.io/blog/2026/specdec/"><![CDATA[<p>This blog is a description of my attempt to implement batched speculative decoding using a target-draft setup. I tried not to make this “yet another blog” on speculative decoding!</p> <h2 id="background">Background</h2> <p>Large Language Models (LLMs) generate text autoregressively, one token at a time. Each token requires a full forward pass through the model, making generation memory-bandwidth bound rather than compute-bound. The model weights must be loaded from memory for every single token, regardless of batch size.</p> <p>Speculative decoding accelerates this process using a simple insight: given context, some tokens are easier to predict than others, such as code syntax, or the answer to <code class="language-plaintext highlighter-rouge">1+1=</code>. A smaller, faster “draft” model can propose multiple tokens that a larger “target” model verifies in parallel. When the draft model’s predictions align with what the target would have produced, we get multiple tokens from a single target forward pass. The ability of the transformer to generate logits for an input sequence in parallel is what we exploit to get a speed-up in wall clock time.</p> <h2 id="the-basic-algorithm">The Basic Algorithm</h2> <ol> <li><strong>Draft Phase:</strong> A small model generates γ (gamma) candidate tokens autoregressively</li> <li><strong>Verify Phase:</strong> The target model processes all γ tokens in one forward pass</li> <li><strong>Accept/Reject:</strong> Using rejection sampling, accept tokens where draft matches target distribution</li> <li><strong>Bonus Token:</strong> Sample one additional token from the target’s distribution</li> </ol> <p>The key mathematical guarantee: the output distribution is <em>identical</em> to running the target model alone. We’re not approximating—we’re getting exact samples faster. (refer paper)</p> <h3 id="rejection-sampling">Rejection Sampling</h3> <p>For each drafted token, we compare probabilities:</p> <ul> <li><code class="language-plaintext highlighter-rouge">p(x)</code> = target model’s probability for token x</li> <li><code class="language-plaintext highlighter-rouge">q(x)</code> = draft model’s probability for token x</li> </ul> <p>Accept the token with probability <code class="language-plaintext highlighter-rouge">min(1, p(x)/q(x))</code>. If rejected at position k:</p> <ul> <li>Discard tokens k onwards</li> <li>Sample a new token from <code class="language-plaintext highlighter-rouge">max(0, p - q)</code> (the “residual” distribution)</li> </ul> <p>This ensures we never accept tokens the target model wouldn’t have produced.</p> <hr/> <h2 id="batched-implementation-challenges">Batched Implementation Challenges</h2> <p>Speculative decoding seems simple enough to implement for B=1. How do we exploit batching to process multiple sequences at once?</p> <p>Extending speculative decoding to batches introduces significant complexity. The core issue: <strong>different sequences accept different numbers of tokens</strong>.</p> <p>Consider a batch of 3 sequences with γ=5:</p> <ul> <li><strong>Sequence 0:</strong> accepts 5 tokens (all drafts correct)</li> <li><strong>Sequence 1:</strong> accepts 2 tokens (rejection at position 3)</li> <li><strong>Sequence 2:</strong> accepts 4 tokens</li> </ul> <p>After one iteration, sequences have different lengths. How do we handle this efficiently?</p> <h3 id="two-approaches">Two Approaches</h3> <p><strong>Approach 1: Per-Sequence Tracking (v1)</strong></p> <ul> <li>Track individual <code class="language-plaintext highlighter-rouge">start_positions</code> per sequence</li> <li>Process only active sequences in draft loop</li> <li>Use DynamicCache with pruning to minimum length</li> <li>Sequences “catch up” by processing variable token ranges</li> </ul> <p><strong>Approach 2: Synchronized Batch Position (v2)</strong></p> <ul> <li>Single <code class="language-plaintext highlighter-rouge">batch_position</code> for all sequences</li> <li>Use attention masking to handle “gaps” from rejections</li> <li>Place bonus tokens at max accepted position</li> <li>Simpler indexing but requires careful mask management</li> </ul> <hr/> <h2 id="the-v2-implementation">The v2 Implementation</h2> <h3 id="design-philosophy">Design Philosophy</h3> <p>v2 uses a synchronized <code class="language-plaintext highlighter-rouge">batch_position</code> pointer and StaticCache. All sequences share the same position in the tensor, with attention masks handling the variable-length reality.</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>After iteration with different acceptance rates:
Sequence 0: [prompt...][tok][tok][tok][tok][tok][bonus][ ][ ]
Sequence 1: [prompt...][tok][tok][PAD][PAD][PAD][bonus][ ][ ]
Sequence 2: [prompt...][tok][tok][tok][tok][PAD][bonus][ ][ ]
                                                   ^
                                            batch_position
</code></pre></div></div> <p>Rejected positions are masked out (attention_mask=0), creating “gaps” in sequences.</p> <h3 id="position-ids-with-gaps">Position IDs with Gaps</h3> <p>With gaps in sequences, position IDs must reflect <em>actual content positions</em>, not tensor indices. We compute this via cumulative sum of the attention mask:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pos_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">attn_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_len</span><span class="p">].</span><span class="nf">cumsum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div> <p>For a mask like <code class="language-plaintext highlighter-rouge">[1,1,1,0,0,1,1]</code>, this gives positions <code class="language-plaintext highlighter-rouge">[0,1,2,2,2,3,4]</code>. The model sees contiguous positions despite gaps in the tensor.</p> <h3 id="the-draft-loop">The Draft Loop</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">draft_steps</span><span class="p">):</span>
    <span class="n">draft_output</span> <span class="o">=</span> <span class="nf">draft_model</span><span class="p">(</span>
        <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="n">batch_position</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">+</span> <span class="n">k</span><span class="p">].</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">batch_position</span> <span class="o">+</span> <span class="n">k</span><span class="p">],</span>
        <span class="n">cache_position</span><span class="o">=</span><span class="n">cache_position</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span>
        <span class="n">position_ids</span><span class="o">=</span><span class="n">pos_id</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span>
        <span class="n">past_key_values</span><span class="o">=</span><span class="n">draft_cache</span><span class="p">,</span>
    <span class="p">)</span>
    
    <span class="c1"># Sample from draft distribution
</span>    <span class="n">Q</span><span class="p">[</span><span class="n">active_indices</span><span class="p">,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="nf">logits_processor</span><span class="p">(</span><span class="n">draft_logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="n">new_token</span> <span class="o">=</span> <span class="n">logits_processor</span><span class="p">.</span><span class="nf">sample</span><span class="p">(</span><span class="n">Q</span><span class="p">[</span><span class="n">active_indices</span><span class="p">,</span> <span class="n">k</span><span class="p">])</span>
    
    <span class="c1"># Place token and update mask
</span>    <span class="n">input_ids</span><span class="p">[</span><span class="n">active_indices</span><span class="p">,</span> <span class="n">batch_position</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_token</span>
    <span class="n">attn_mask</span><span class="p">[</span><span class="n">active_indices</span><span class="p">,</span> <span class="n">batch_position</span> <span class="o">+</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
</code></pre></div></div> <p>Each step:</p> <ol> <li>Feeds the previous token (at <code class="language-plaintext highlighter-rouge">batch_position - 1 + k</code>)</li> <li>Extends attention mask to include new position</li> <li>Stores draft probabilities in Q matrix for later comparison</li> <li>Places drafted token at <code class="language-plaintext highlighter-rouge">batch_position + k</code></li> </ol> <h3 id="parallel-verification">Parallel Verification</h3> <p>The target model verifies all drafted tokens in one forward pass:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">target_output</span> <span class="o">=</span> <span class="nf">target_model</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="n">batch_position</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">batch_position</span><span class="o">+</span><span class="n">draft_steps</span><span class="p">],</span>
    <span class="n">attention_mask</span><span class="o">=</span><span class="n">attn_mask</span><span class="p">[:,</span> <span class="p">:</span><span class="n">batch_position</span><span class="o">+</span><span class="n">draft_steps</span><span class="p">],</span>
    <span class="n">cache_position</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="n">batch_position</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_position</span><span class="o">+</span><span class="n">draft_steps</span><span class="p">),</span>
    <span class="n">position_ids</span><span class="o">=</span><span class="n">pos_id</span><span class="p">[:,</span> <span class="n">batch_position</span><span class="o">-</span><span class="mi">1</span><span class="p">:</span><span class="n">batch_position</span><span class="o">+</span><span class="n">draft_steps</span><span class="p">],</span>
    <span class="n">past_key_values</span><span class="o">=</span><span class="n">target_cache</span><span class="p">,</span>
<span class="p">)</span>
</code></pre></div></div> <p>This processes <code class="language-plaintext highlighter-rouge">draft_steps + 1</code> tokens:</p> <ul> <li>Position <code class="language-plaintext highlighter-rouge">batch_position - 1</code>: the last accepted token (to get logits for first draft)</li> <li>Positions <code class="language-plaintext highlighter-rouge">batch_position</code> to <code class="language-plaintext highlighter-rouge">batch_position + draft_steps - 1</code>: the drafted tokens</li> </ul> <h3 id="vectorized-acceptance">Vectorized Acceptance</h3> <p>Instead of sequential rejection sampling, we vectorize:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Gather probabilities for drafted tokens
</span><span class="n">p_tok</span> <span class="o">=</span> <span class="n">p</span><span class="p">[:,</span> <span class="p">:</span><span class="n">draft_steps</span><span class="p">].</span><span class="nf">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">drafted_tokens</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">q_tok</span> <span class="o">=</span> <span class="n">q</span><span class="p">[:,</span> <span class="p">:</span><span class="n">draft_steps</span><span class="p">].</span><span class="nf">gather</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">drafted_tokens</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)).</span><span class="nf">squeeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Log-space rejection sampling
</span><span class="n">log_ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">p_tok</span><span class="p">)</span> <span class="o">-</span> <span class="n">torch</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="n">q_tok</span><span class="p">)</span>
<span class="n">log_r</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">empty_like</span><span class="p">(</span><span class="n">log_ratio</span><span class="p">).</span><span class="nf">uniform_</span><span class="p">().</span><span class="nf">log_</span><span class="p">()</span>

<span class="c1"># Cumulative product finds first rejection
</span><span class="n">acceptance_status</span> <span class="o">=</span> <span class="p">(</span><span class="n">log_r</span> <span class="o">&lt;=</span> <span class="n">log_ratio</span><span class="p">).</span><span class="nf">cumprod</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">).</span><span class="nf">bool</span><span class="p">()</span>
<span class="n">num_accepted</span> <span class="o">=</span> <span class="n">acceptance_status</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p>The <code class="language-plaintext highlighter-rouge">cumprod</code> trick: once we hit a rejection (False), all subsequent positions become False, giving us the count of consecutively accepted tokens.</p> <h3 id="bonus-token-placement">Bonus Token Placement</h3> <p>Here’s where batching gets tricky. Different sequences accept different amounts:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">batch_pos_shift</span> <span class="o">=</span> <span class="n">accepted_draft_length</span><span class="p">.</span><span class="nf">max</span><span class="p">().</span><span class="nf">item</span><span class="p">()</span>
<span class="n">input_ids</span><span class="p">[</span><span class="n">active_indices</span><span class="p">,</span> <span class="n">batch_position</span> <span class="o">+</span> <span class="n">batch_pos_shift</span><span class="p">]</span> <span class="o">=</span> <span class="n">extra_tokens</span>
<span class="n">attn_mask</span><span class="p">[</span><span class="n">active_indices</span><span class="p">,</span> <span class="n">batch_position</span> <span class="o">+</span> <span class="n">batch_pos_shift</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">batch_position</span> <span class="o">+=</span> <span class="n">batch_pos_shift</span> <span class="o">+</span> <span class="mi">1</span>
</code></pre></div></div> <p>All sequences place their bonus token at the <em>maximum</em> accepted position. Sequences that accepted fewer tokens have gaps between their last accepted token and the bonus. These gaps are masked out, so attention skips them.</p> <p><strong>Why place at max position?</strong></p> <p>If we placed bonus tokens at each sequence’s individual position, the next iteration’s input would be at different tensor indices per sequence, breaking batched processing. By aligning to max position, <code class="language-plaintext highlighter-rouge">batch_position</code> remains synchronized.</p> <hr/> <h2 id="huggingface-cache-challenges">HuggingFace Cache Challenges</h2> <h3 id="staticcache-vs-dynamiccache">StaticCache vs DynamicCache</h3> <p>HuggingFace Transformers offers two KV-cache implementations:</p> <p><strong>DynamicCache</strong></p> <ul> <li>Grows dynamically as tokens are generated</li> <li>Supports <code class="language-plaintext highlighter-rouge">crop(max_length)</code> to truncate</li> <li>No <code class="language-plaintext highlighter-rouge">cache_position</code> parameter needed—just appends</li> <li>Memory reallocations can cause fragmentation</li> </ul> <p><strong>StaticCache</strong></p> <ul> <li>Pre-allocated to maximum length</li> <li>Requires explicit <code class="language-plaintext highlighter-rouge">cache_position</code> to specify write locations</li> <li>More memory efficient for known max lengths</li> <li>Can “overwrite” positions (useful for our gap scenario)</li> </ul> <h3 id="the-cache-gap-problem">The Cache Gap Problem</h3> <p>With synchronized <code class="language-plaintext highlighter-rouge">batch_position</code>, a subtle bug emerges:</p> <ul> <li><strong>Draft loop</strong> processes positions <code class="language-plaintext highlighter-rouge">[batch_position-1, batch_position+draft_steps-2]</code></li> <li><strong>Target verification</strong> processes positions <code class="language-plaintext highlighter-rouge">[batch_position-1, batch_position+draft_steps-1]</code></li> </ul> <p>The target processes <em>one more position</em> than the draft loop. After the iteration:</p> <ul> <li>Target cache has KVs for positions 0 to <code class="language-plaintext highlighter-rouge">batch_position + draft_steps - 1</code></li> <li>Draft cache has KVs for positions 0 to <code class="language-plaintext highlighter-rouge">batch_position + draft_steps - 2</code></li> </ul> <p><strong>Gap: one position missing in draft cache.</strong></p> <p>In the next iteration, when <code class="language-plaintext highlighter-rouge">batch_position</code> advances, the draft model tries to attend to a position that was never computed—containing garbage values.</p> <h3 id="why-v1-doesnt-have-this-problem">Why v1 Doesn’t Have This Problem</h3> <p>v1 uses per-sequence tracking and processes token <em>ranges</em>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">draft_output</span> <span class="o">=</span> <span class="nf">draft_model</span><span class="p">(</span>
    <span class="n">input_ids</span><span class="o">=</span><span class="n">input_ids</span><span class="p">[</span><span class="n">active_indices</span><span class="p">,</span> <span class="n">min_seq_len</span><span class="o">+</span><span class="n">k</span><span class="p">:</span><span class="n">max_length</span><span class="o">+</span><span class="n">k</span><span class="p">],</span>
    <span class="bp">...</span>
<span class="p">)</span>
</code></pre></div></div> <p>When sequences have different lengths, <code class="language-plaintext highlighter-rouge">max_length - min_seq_len</code> can be &gt; 1, processing multiple tokens per step. The cache naturally “catches up” because sequences behind process more tokens.</p> <h3 id="solutions-for-v2">Solutions for v2</h3> <p><strong>Option 1: Extra Forward Pass</strong></p> <p>After the draft loop, run one more draft forward to fill the gap:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nf">draft_model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="n">batch_position</span> <span class="o">+</span> <span class="n">draft_steps</span> <span class="o">-</span> <span class="mi">1</span><span class="p">],</span> <span class="p">...)</span>
</code></pre></div></div> <p>Simple but adds latency.</p> <p><strong>Option 2: Process 2 Tokens on First Step</strong></p> <p>On k=0 of the next iteration, process 2 tokens instead of 1 to fill the gap:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">draft_cache_len</span> <span class="o">&lt;</span> <span class="n">batch_position</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
    <span class="nf">draft_model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="n">draft_cache_len</span><span class="p">:</span><span class="n">batch_position</span><span class="p">],</span> <span class="p">...)</span>
</code></pre></div></div> <p>Same total compute, just redistributed.</p> <p><strong>Option 3: Use DynamicCache with Cropping</strong></p> <p>Crop both caches to <code class="language-plaintext highlighter-rouge">batch_position - 1</code> at iteration start, then process normally. The cache regrows each iteration.</p> <h3 id="position-id-alignment">Position ID Alignment</h3> <p>With StaticCache, <code class="language-plaintext highlighter-rouge">cache_position</code> determines <em>where</em> KVs are stored, while <code class="language-plaintext highlighter-rouge">position_ids</code> determines RoPE embeddings. These can differ!</p> <p>For sequences with gaps:</p> <ul> <li>Cache position might be 15 (tensor index)</li> <li>Position ID might be 13 (actual content position after gaps)</li> </ul> <p>The KV is stored at index 15, but RoPE is computed for position 13. When later attending, the query’s RoPE (based on its position ID) correctly matches the key’s embedded RoPE.</p> <p>This works because RoPE is “baked into” K at storage time. The cache index is just for storage/retrieval—the positional information is in the embeddings.</p> <h3 id="attention-mask-interactions">Attention Mask Interactions</h3> <p>The attention mask must correctly reflect which positions are valid:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">attn_mask</span><span class="p">[</span><span class="n">rejected_indices</span><span class="p">,</span> <span class="n">rejected_positions</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>  <span class="c1"># Mask out rejections
</span></code></pre></div></div> <p>When computing attention, masked positions contribute zero to the softmax. Even if garbage exists in the cache at those positions, it’s ignored.</p> <p><strong>Caution</strong>: The mask must be consistent between draft and target models. If the target used a different mask during verification, the cached KVs might have different attention patterns than what the draft expects.</p> <hr/> <h2 id="performance-characteristics">Performance Characteristics</h2> <h3 id="acceptance-rate">Acceptance Rate</h3> <p>The acceptance rate—proportion of drafted tokens accepted—determines speedup:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Speedup ≈ (accepted_tokens + 1) / (draft_cost + target_cost)
</code></pre></div></div> <p>Factors affecting acceptance rate:</p> <ul> <li>Draft/target model alignment (same family helps)</li> <li>Task difficulty (factual recall vs creative writing)</li> <li>Temperature (lower = more deterministic = higher acceptance)</li> </ul> <h3 id="block-efficiency">Block Efficiency</h3> <p>We track “block efficiency”—tokens generated per speculation block:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">block_efficiency</span> <span class="o">=</span> <span class="n">total_tokens</span> <span class="o">/</span> <span class="n">num_blocks</span>
</code></pre></div></div> <p>With γ=5, perfect acceptance gives efficiency of 6 (5 drafts + 1 bonus). Real-world values of 3-4 are common.</p> <h3 id="memory-bandwidth">Memory Bandwidth</h3> <p>Speculative decoding shines when memory-bound. The target model’s weights are loaded once to verify γ tokens instead of γ times. For large models on consumer GPUs, this is significant.</p> <hr/> <h2 id="conclusion">Conclusion</h2> <p>Batched speculative decoding requires careful orchestration of:</p> <ul> <li>Synchronized position tracking across variable-length sequences</li> <li>Attention masks that correctly handle gaps from rejections</li> <li>KV-caches that align between draft and target models</li> <li>Position IDs computed from actual content, not tensor indices</li> </ul> <p>The HuggingFace cache abstractions (StaticCache/DynamicCache) provide building blocks, but their semantics around <code class="language-plaintext highlighter-rouge">cache_position</code> vs <code class="language-plaintext highlighter-rouge">position_ids</code> require careful handling. The gap problem—where draft and target caches diverge by one position—is a subtle bug that manifests as degraded acceptance rates rather than obvious failures.</p> <p>Understanding these mechanics enables building efficient, correct batched speculative decoding systems that maintain the mathematical guarantees of the original algorithm while leveraging GPU parallelism.</p>]]></content><author><name></name></author><category term="projects"/><category term="AI"/><category term="LLMs"/><category term="inference"/><summary type="html"><![CDATA[Sneak peek into batched speculative decoding in LLMs]]></summary></entry></feed>